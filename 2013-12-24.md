# ML Research

It's the day before Christmas, and I'm taking notes on what I've researched.

## word2vec

I managed to get word2vec with python working on one of my digitalocean instances. One problem I ran into (and still have) is a bias in the training data that's not present in the examples given. In particular, there is a 
[gensim tutorial](http://radimrehurek.com/gensim/models/word2vec.html) that shows this example:

```python
>>> model.most_similar(positive=['woman', 'king'], negative=['man'])
[('queen', 0.50882536), ...]
```

However, I get back something like this:

```python
>>> model.most_similar(positive=['woman', 'king'], negative=['man'])
[('burgundy', 0.54468679), ('viii', 0.5410071), ('queen', 0.5312984), ('afonso', 0.53091687), ('consort', 0.52081859), ('aragon', 0.51921749), ('vii', 0.51237959), ('anshan', 0.50568104), ('iv', 0.50558347), ('castile', 0.50518328)]
```

So queen is in there, but it's not as high of a score as I thought I'd get. It seems to be skew towards something mentioning "castile" and "aragon" (maybe Tolkein?). I'm not 100% sure. 

I'd like to download the entire Wikipedia compressed text, but that's several gigabytes, so I may have to come up with MapReduce job to handle that data.

# Crypto Research

An interesting blogpost about how to implement crypto popped up on reddit today. [So, you want to crypto](http://blog.existentialize.com/so-you-want-to-crypto.html) is a very link-baitish title, but I worked on me. One
particularly interesting link is to a Bruce Schneier book [Cryptograpy Engineering](https://www.schneier.com/book-ce.html) which seems like something I would read, if I had 2 weeks of my life to do whatever I wanted. 
