= ML Research

It's the day before Christmas, and I'm taking notes on what I've researched.

== word2vec

I managed to get word2vec with python working on one of my digitalocean instances. One problem I ran into (and still have) is a bias
in the training data that's not present in the examples given. In particular, there is a 
[gensim tutorial](http://radimrehurek.com/gensim/models/word2vec.html) that shows this example:

```python
>>> model.most_similar(positive=['woman', 'king'], negative=['man'])
[('queen', 0.50882536), ...]
```

However, I get back something like this:




= Crypto Research

